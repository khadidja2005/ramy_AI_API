{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "W1HaSJy1_M1n"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torchvision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import shutil\n",
        "from difflib import get_close_matches\n",
        "\n",
        "def preprocess_name(name):\n",
        "    \"\"\"\n",
        "    Standardize product names for better matching:\n",
        "      - Convert to lowercase\n",
        "      - Replace abbreviations (before removing punctuation)\n",
        "      - Remove special characters\n",
        "    \"\"\"\n",
        "    # Convert to lowercase for consistency\n",
        "    name = name.lower()\n",
        "\n",
        "    # Abbreviation mappings\n",
        "    replacements = {\n",
        "        'c.gaz': 'cannete_gazifiée',\n",
        "        'c.jus': 'cannete_jus',\n",
        "        'or': 'orange',\n",
        "        'pe': 'peche',\n",
        "        'abr': 'abricot',\n",
        "        'fr': 'fraise',\n",
        "        'pom': 'pomme',\n",
        "        'ban': 'banane',\n",
        "        'mang': 'mangue',\n",
        "        'gren': 'grenadine',\n",
        "        'cit': 'citron',\n",
        "        'c.malt': 'cannete_maltée',\n",
        "        'w.f': 'water_fruits'\n",
        "    }\n",
        "\n",
        "    # Replace abbreviations first (keeping punctuation so that patterns match)\n",
        "    for abbr, full in replacements.items():\n",
        "        name = re.sub(rf'\\b{re.escape(abbr)}\\b', full, name)\n",
        "\n",
        "    # Now remove special characters (including dots, spaces, underscores, etc.)\n",
        "    name = re.sub(r'[^a-z0-9]', '', name)\n",
        "\n",
        "    return name\n",
        "\n",
        "def get_simplified_famille(filename):\n",
        "    \"\"\"\n",
        "    Classify products into three main categories: PET, Cannete, and Autres\n",
        "\n",
        "    Args:\n",
        "        filename: The image filename\n",
        "    Returns:\n",
        "        str: One of 'PET', 'Cannete', or 'Autres'\n",
        "    \"\"\"\n",
        "    filename_lower = filename.lower()\n",
        "    processed_name = preprocess_name(filename_lower)\n",
        "\n",
        "    # Check for PET products\n",
        "    if 'pet' in processed_name or any(term in processed_name for term in ['energy', 'milk', 'lben']):\n",
        "        return 'PET'\n",
        "\n",
        "    # Check for Cannete products\n",
        "    if 'cannete' in processed_name or any(term in processed_name for term in ['c.gaz', 'c.jus', 'c.malt']):\n",
        "        return 'Cannete'\n",
        "\n",
        "    # Everything else goes to Autres\n",
        "    return 'Pack'\n",
        "\n",
        "def group_images_by_simplified_famille(source_folder, destination_folder):\n",
        "    \"\"\"\n",
        "    Group images into three main folders: PET, Cannete, and Autres\n",
        "\n",
        "    Args:\n",
        "        source_folder: Folder containing images\n",
        "        destination_folder: Where to create classification folders\n",
        "    \"\"\"\n",
        "    if not os.path.exists(destination_folder):\n",
        "        os.makedirs(destination_folder)\n",
        "\n",
        "    # Track results\n",
        "    results = {'PET': [], 'Cannete': [], 'Pack': []}\n",
        "\n",
        "    # Create the three main folders\n",
        "    for famille in ['PET', 'Cannete', 'Pack']:\n",
        "        famille_folder = os.path.join(destination_folder, famille)\n",
        "        if not os.path.exists(famille_folder):\n",
        "            os.makedirs(famille_folder)\n",
        "\n",
        "    # Process each image\n",
        "    for filename in os.listdir(source_folder):\n",
        "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "            famille = get_simplified_famille(filename)\n",
        "\n",
        "            # Move the file to appropriate folder\n",
        "            source_path = os.path.join(source_folder, filename)\n",
        "            dest_path = os.path.join(destination_folder, famille, filename)\n",
        "            shutil.move(source_path, dest_path)\n",
        "            results[famille].append(filename)\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\nClassification Results:\")\n",
        "    for famille, files in results.items():\n",
        "        print(f\"\\n{famille} ({len(files)} files):\")\n",
        "        for filename in files:\n",
        "            print(f\"- {filename}\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "MfWe-qleaDI1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "source_folder = \"/content\"\n",
        "destination_folder = \"/content/classes\"\n",
        "\n",
        "results = group_images_by_simplified_famille(source_folder, destination_folder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iq94pIB5PrVG",
        "outputId": "589398e7-394f-4389-8cbf-0ab9bc448346"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Results:\n",
            "\n",
            "PET (54 files):\n",
            "- PET_Frutty_Orange_Peche_2L.png\n",
            "- ENERGY RED BUF MENTH 33CL..png\n",
            "- ramy LBEN.png\n",
            "- ENERGY BUF 33CL.png\n",
            "- PET_Malt_Miel_33cl.png\n",
            "- PET_WaterFruits_Agrumes_33cl.png\n",
            "- PET_Malt_Ananas_33cl.png\n",
            "- PET_Ramy_Orange_Ananas_1,25L.png\n",
            "- MILKY PET 1L.png\n",
            "- PET_Ramy_Cocktail_Mangue_1,25L.png\n",
            "- PET_Energie_Classique_33cl.png\n",
            "- PET_Ramy_Pomme_Banane_1,25L.png\n",
            "- PET_Ramy_Mandarine_1,25L.png\n",
            "- PET_Ramy_Cocktail_Mure_1,25L.png\n",
            "- PET_Ramy_Grenadine_1,25L.png\n",
            "- PET_Extra_Orange_2L.png\n",
            "- PET_Malt_MFruits_33cl.png\n",
            "- PET_Ramy_Trio_Fraise_Pomme_Banane_1,25L.png\n",
            "- PET_Malt_Citron_33cl(1).png\n",
            "- PET_WaterFruits_Mojito_33cl.png\n",
            "- PET_Energie_Classique_33cl(1).png\n",
            "- PET_Ramy_Fraise_1,25L.png\n",
            "- PET_Ramy_Citron_1,25L.png\n",
            "- ramy LBEN BIF.png\n",
            "- PET_Extra_Orange__Peche_Fraise_2L.png\n",
            "- PET_Malt_Ananas_33cl(1).png\n",
            "- PET_Ramy_Orange_Peche_Fraise_1,25L.png\n",
            "- PET_Energie_Power_Fruits_33cl.png\n",
            "- ENERGY DRINK 33CL.png\n",
            "- PET_Ramy_Cerise_1,25L.png\n",
            "- PET_Energie_Miel_33cl.png\n",
            "- PET_Malt_Citron_33cl.png\n",
            "- PET_Frutty_Ananas_2L.png\n",
            "- PET_Ramy_Cocktail_Passion_1,25L.png\n",
            "- PET_WaterFruits_Pomme_Banane_33cl.png\n",
            "- PET_Extra_Ananas_2L.png\n",
            "- PET_Energie_Menthe_33cl.png\n",
            "- PET_Malt_MFruits_33cl(1).png\n",
            "- PET_WaterFruits_Pomme_33cl.png\n",
            "- PET_Frutty_Orange_Peche_Fraise_2L.png\n",
            "- PET_Ramy_Orange_Mangue_1,25L.png\n",
            "- PET_Malt_Peche_33cl.png\n",
            "- PET_Frutty_Orange_Abricot_2L.png\n",
            "- PET_Ramy_Peche_1,25L.png\n",
            "- PET_Frutty_Passion_2L.png\n",
            "- PET_Ramy_Cocktail_Raisin_1,25L.png\n",
            "- PET_Ramy_ananas_1,25L.png\n",
            "- PET_Ramy_Gazifiée_Orange_33cl.png\n",
            "- ENERGY POWER FRUITD 33CL.png\n",
            "- PET_WaterFruits_Ananas_33cl.png\n",
            "- PET_Ramy_Orange_Abricot_1,25L.png\n",
            "- PET_Ramy_Orange_Sanguine_Grenadine_1,25L.png\n",
            "- PET_WaterFruits_Grenadine_33cl.png\n",
            "- PET_Frutty_Orange_2L.png\n",
            "\n",
            "Cannete (16 files):\n",
            "- Cannete_Jus_Orange_24cl.png\n",
            "- Cannete_Gazifiée_Orange_33cl.png\n",
            "- Cannete_Maltée_Peche_33cl.png\n",
            "- Cannete_Maltée_Mojito_33cl.png\n",
            "- Cannete_Jus_Citron_24cl.png\n",
            "- Cannete_Jus_Ananas_24cl.png\n",
            "- Cannete_Gazifiée_Ananas_33cl.png\n",
            "- Cannete_Jus_Cocktail_24cl.png\n",
            "- Cannete_Maltée_Ananas_33cl.png\n",
            "- Cannete_Jus_Orange_Abricot_24cl.png\n",
            "- Cannete_Maltée_Miel_33cl.png\n",
            "- Cannete_Gazifiée_Agrumes_33cl.png\n",
            "- Cannete_Jus_Orange_Peche_24cl.png\n",
            "- Cannete_Gazifiée_Citron_Menthe_33cl.png\n",
            "- Cannete_Jus_Fraise_24cl.png\n",
            "- Cannete_Gazifiée_Zest_33cl.png\n",
            "\n",
            "Autres (41 files):\n",
            "- Pack_Ramy_Orange_Peche_1L.png\n",
            "- Pack_Frutty_Ananas_1L.png\n",
            "- Pack_Frutty_Orange_Peche_1L.png\n",
            "- extra_orange_abricot30cl.png\n",
            "- Pack_Ramy_Orange_Light_1L.png\n",
            "- Pack_Kids_Orange_125ml.png\n",
            "- Pack_Ramy_Orange_Abricot_1L.png\n",
            "- UP_Choco_125ml.png\n",
            "- Pack_Frutty_Tropical_1L.png\n",
            "- Pack_Ramy_Cocktail_Mangue_1L.png\n",
            "- Pack_Kids_Cocktail_125ml.png\n",
            "- UP_Banane_20cl.png\n",
            "- Pack_Frutty_Kids_Orange_Abricot_20cl.png\n",
            "- Pack_Frutty_Kids_Ananas_20cl.png\n",
            "- UP_Fraise_20cl.png\n",
            "- Pack_Frutty_Kids_Orange_20cl.png\n",
            "- Pack_Frutty_Kids_Cocktail_20cl.png\n",
            "- Pack_Ramy_Orange_Abricot_2L.png\n",
            "- Pack_Ramy_Orange_20cl.png\n",
            "- UP_Choco_20cl.png\n",
            "- Pack_Ramy_Orange_Peche_20cl.png\n",
            "- ramy canette jus 204ml.png\n",
            "- Pack_Frutty_Orange_1L.png\n",
            "- extra_ananas30cl.png\n",
            "- Pack_Kids_Peche_125ml.png\n",
            "- ramy-jus-d-orange-et-pêche-2-l.jpg\n",
            "- Pack_Ramy_Orange_1L.png\n",
            "- UP_Banane_125ml.png\n",
            "- Pack_Ramy_Ananas_1L.png\n",
            "- Pack_Ramy_Ananas_20cl.png\n",
            "- Pack_Kids_Ananas_125ml.png\n",
            "- Pack_Frutty_Orange_Abricot_1L.png\n",
            "- Pack_Frutty_Kids_Orange_Peche_20cl.png\n",
            "- Pack_Frutty_Cocktail_1L.png\n",
            "- ramy-jus-d-orange-et-pêche-2-__l.jpg\n",
            "- Pack_Kids_Fraise_125ml.png\n",
            "- UP_Fraise_125ml.png\n",
            "- Pack_Ramy_Mandarine_1L.png\n",
            "- Pack_Ramy_Orange_Peche_2L.png\n",
            "- extra_orange_peche_fraise30cl.png\n",
            "- ramy pack 2L.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U albumentations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXZ84tOK5Bna",
        "outputId": "496cdf64-6ead-4a54-9d54-d7469c9a274d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.11/dist-packages (2.0.4)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.11/dist-packages (from albumentations) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from albumentations) (1.13.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from albumentations) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.10.6)\n",
            "Requirement already satisfied: albucore==0.0.23 in /usr/local/lib/python3.11/dist-packages (from albumentations) (0.0.23)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from albumentations) (4.11.0.86)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.23->albumentations) (3.11.3)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.23->albumentations) (6.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from PIL import Image\n",
        "\n",
        "# Import Albumentations and its PyTorch converter\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# --- 1. Create a Custom Dataset that uses Albumentations ---\n",
        "class AlbumentationsDataset(ImageFolder):\n",
        "    def __init__(self, root, transform=None):\n",
        "        # Filter out hidden directories\n",
        "        self.root = root\n",
        "        valid_classes = [d for d in os.listdir(root)\n",
        "                        if os.path.isdir(os.path.join(root, d)) and not d.startswith('.')]\n",
        "\n",
        "        self.classes = sorted(valid_classes)\n",
        "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
        "\n",
        "        # Get all valid image files\n",
        "        self.samples = []\n",
        "        self.targets = []\n",
        "\n",
        "        valid_extensions = ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp')\n",
        "\n",
        "        for class_name in valid_classes:\n",
        "            class_idx = self.class_to_idx[class_name]\n",
        "            class_dir = os.path.join(root, class_name)\n",
        "\n",
        "            for fname in os.listdir(class_dir):\n",
        "                if not fname.startswith('.') and fname.lower().endswith(valid_extensions):\n",
        "                    path = os.path.join(class_dir, fname)\n",
        "                    self.samples.append((path, class_idx))\n",
        "                    self.targets.append(class_idx)\n",
        "\n",
        "        self.targets = np.array(self.targets)\n",
        "        self.albumentations_transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        path, target = self.samples[index]\n",
        "        # Open image and convert to RGB (as numpy array)\n",
        "        image = np.array(Image.open(path).convert(\"RGB\"))\n",
        "        if self.albumentations_transform:\n",
        "            augmented = self.albumentations_transform(image=image)\n",
        "            image = augmented[\"image\"]\n",
        "        return image, target\n",
        "\n",
        "# --- 2. Define Advanced Augmentations using Albumentations ---\n",
        "train_transform = A.Compose([\n",
        "    A.Resize(height=256, width=256),\n",
        "    A.CenterCrop(height=224, width=224),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.2),\n",
        "    A.Rotate(limit=30, p=0.5),\n",
        "    A.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225],\n",
        "        max_pixel_value=255.0\n",
        "    ),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "val_transform = A.Compose([\n",
        "    A.Resize(height=256, width=256),\n",
        "    A.CenterCrop(height=224, width=224),\n",
        "    A.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225],\n",
        "        max_pixel_value=255.0\n",
        "    ),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "def mixup_collate_fn(batch, alpha=0.4):\n",
        "    \"\"\"\n",
        "    Applies MixUp augmentation on a batch.\n",
        "    \"\"\"\n",
        "    images, labels = zip(*batch)\n",
        "    images = torch.stack(images)\n",
        "    labels = torch.tensor(labels)\n",
        "\n",
        "    lam = np.random.beta(alpha, alpha) if alpha > 0 else 1.0\n",
        "    batch_size = images.size(0)\n",
        "    index = torch.randperm(batch_size)\n",
        "\n",
        "    mixed_images = lam * images + (1 - lam) * images[index, :]\n",
        "    labels_a, labels_b = labels, labels[index]\n",
        "    return mixed_images, labels_a, labels_b, lam\n",
        "\n",
        "def train_model(data_dir, num_epochs=25, batch_size=32, learning_rate=0.001):\n",
        "    # Load and split dataset\n",
        "    full_dataset = AlbumentationsDataset(data_dir, transform=train_transform)\n",
        "\n",
        "    # Calculate split sizes\n",
        "    train_size = int(0.8 * len(full_dataset))\n",
        "    val_size = len(full_dataset) - train_size\n",
        "\n",
        "    # Create train and validation splits\n",
        "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "    # Override the transform for validation dataset\n",
        "    val_dataset.dataset.albumentations_transform = val_transform\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=4,\n",
        "        collate_fn=lambda batch: mixup_collate_fn(batch, alpha=0.4)\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=4\n",
        "    )\n",
        "\n",
        "    # Setup device\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Setup model\n",
        "    model = models.resnet50(pretrained=True)\n",
        "    num_classes = len(full_dataset.classes)\n",
        "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Print class information\n",
        "    print(f\"Classes found: {full_dataset.classes}\")\n",
        "    print(f\"Class to idx mapping: {full_dataset.class_to_idx}\")\n",
        "\n",
        "    # Setup training\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        for inputs, labels_a, labels_b, lam in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels_a = labels_a.to(device)\n",
        "            labels_b = labels_b.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # MixUp loss\n",
        "            loss = lam * criterion(outputs, labels_a) + (1 - lam) * criterion(outputs, labels_b)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            running_corrects += torch.sum(preds == labels_a.data)\n",
        "\n",
        "        epoch_loss = running_loss / train_size\n",
        "        epoch_acc = running_corrects.double() / train_size\n",
        "\n",
        "        print(f'Training Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_corrects = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                val_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        epoch_val_loss = val_loss / val_size\n",
        "        epoch_val_acc = val_corrects.double() / val_size\n",
        "\n",
        "        print(f'Validation Loss: {epoch_val_loss:.4f} Acc: {epoch_val_acc:.4f}')\n",
        "\n",
        "        # Save best model\n",
        "        if epoch_val_acc > best_val_acc:\n",
        "            best_val_acc = epoch_val_acc\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    print('Training completed!')\n",
        "    return model\n",
        "\n",
        "# Usage\n",
        "if __name__ == \"__main__\":\n",
        "    data_dir = '/content/classes'  # Update this path to your dataset directory\n",
        "    model = train_model(data_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LKxQCqR-enS",
        "outputId": "c007fb92-a6f8-4e60-e823-145edd94d577"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 182MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes found: ['Cannete', 'PET', 'Pack']\n",
            "Class to idx mapping: {'Cannete': 0, 'PET': 1, 'Pack': 2}\n",
            "\n",
            "Epoch 1/25\n",
            "----------\n",
            "Training Loss: 1.0256 Acc: 0.4318\n",
            "Validation Loss: 0.9073 Acc: 0.6522\n",
            "\n",
            "Epoch 2/25\n",
            "----------\n",
            "Training Loss: 0.9090 Acc: 0.5795\n",
            "Validation Loss: 0.7208 Acc: 0.7391\n",
            "\n",
            "Epoch 3/25\n",
            "----------\n",
            "Training Loss: 0.8040 Acc: 0.5795\n",
            "Validation Loss: 0.5685 Acc: 0.8696\n",
            "\n",
            "Epoch 4/25\n",
            "----------\n",
            "Training Loss: 0.7389 Acc: 0.5682\n",
            "Validation Loss: 0.4612 Acc: 0.8696\n",
            "\n",
            "Epoch 5/25\n",
            "----------\n",
            "Training Loss: 0.6308 Acc: 0.5227\n",
            "Validation Loss: 0.3776 Acc: 0.9130\n",
            "\n",
            "Epoch 6/25\n",
            "----------\n",
            "Training Loss: 0.5232 Acc: 0.5795\n",
            "Validation Loss: 0.3213 Acc: 0.9565\n",
            "\n",
            "Epoch 7/25\n",
            "----------\n",
            "Training Loss: 0.6688 Acc: 0.7273\n",
            "Validation Loss: 0.2868 Acc: 1.0000\n",
            "\n",
            "Epoch 8/25\n",
            "----------\n",
            "Training Loss: 0.3576 Acc: 0.8295\n",
            "Validation Loss: 0.2827 Acc: 1.0000\n",
            "\n",
            "Epoch 9/25\n",
            "----------\n",
            "Training Loss: 0.3913 Acc: 0.7500\n",
            "Validation Loss: 0.2791 Acc: 1.0000\n",
            "\n",
            "Epoch 10/25\n",
            "----------\n",
            "Training Loss: 0.3353 Acc: 0.5227\n",
            "Validation Loss: 0.2759 Acc: 1.0000\n",
            "\n",
            "Epoch 11/25\n",
            "----------\n",
            "Training Loss: 0.4860 Acc: 0.9773\n",
            "Validation Loss: 0.2692 Acc: 1.0000\n",
            "\n",
            "Epoch 12/25\n",
            "----------\n",
            "Training Loss: 0.2942 Acc: 0.7727\n",
            "Validation Loss: 0.2616 Acc: 1.0000\n",
            "\n",
            "Epoch 13/25\n",
            "----------\n",
            "Training Loss: 0.4250 Acc: 0.8182\n",
            "Validation Loss: 0.2533 Acc: 1.0000\n",
            "\n",
            "Epoch 14/25\n",
            "----------\n",
            "Training Loss: 0.4446 Acc: 0.9318\n",
            "Validation Loss: 0.2463 Acc: 1.0000\n",
            "\n",
            "Epoch 15/25\n",
            "----------\n",
            "Training Loss: 0.4072 Acc: 0.7841\n",
            "Validation Loss: 0.2485 Acc: 1.0000\n",
            "\n",
            "Epoch 16/25\n",
            "----------\n",
            "Training Loss: 0.2480 Acc: 0.3523\n",
            "Validation Loss: 0.2500 Acc: 1.0000\n",
            "\n",
            "Epoch 17/25\n",
            "----------\n",
            "Training Loss: 0.4004 Acc: 0.4545\n",
            "Validation Loss: 0.2506 Acc: 1.0000\n",
            "\n",
            "Epoch 18/25\n",
            "----------\n",
            "Training Loss: 0.5482 Acc: 0.6023\n",
            "Validation Loss: 0.2515 Acc: 1.0000\n",
            "\n",
            "Epoch 19/25\n",
            "----------\n",
            "Training Loss: 0.3805 Acc: 0.7955\n",
            "Validation Loss: 0.2500 Acc: 1.0000\n",
            "\n",
            "Epoch 20/25\n",
            "----------\n",
            "Training Loss: 0.5062 Acc: 0.5795\n",
            "Validation Loss: 0.2496 Acc: 1.0000\n",
            "\n",
            "Epoch 21/25\n",
            "----------\n",
            "Training Loss: 0.3552 Acc: 0.7614\n",
            "Validation Loss: 0.2483 Acc: 1.0000\n",
            "\n",
            "Epoch 22/25\n",
            "----------\n",
            "Training Loss: 0.5185 Acc: 0.5909\n",
            "Validation Loss: 0.2476 Acc: 1.0000\n",
            "\n",
            "Epoch 23/25\n",
            "----------\n",
            "Training Loss: 0.4107 Acc: 0.6364\n",
            "Validation Loss: 0.2482 Acc: 1.0000\n",
            "\n",
            "Epoch 24/25\n",
            "----------\n",
            "Training Loss: 0.5723 Acc: 0.4545\n",
            "Validation Loss: 0.2490 Acc: 1.0000\n",
            "\n",
            "Epoch 25/25\n",
            "----------\n",
            "Training Loss: 0.4952 Acc: 0.7500\n",
            "Validation Loss: 0.2484 Acc: 1.0000\n",
            "Training completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Assume 'model' is your trained model object\n",
        "with open('model.pkl', 'wb') as f:\n",
        "    pickle.dump(model, f)\n",
        "\n",
        "print(\"Model saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uP_WE_oh_Qbm",
        "outputId": "79ff9f78-cf69-40a3-e64a-b25ac967c194"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved successfully.\n"
          ]
        }
      ]
    }
  ]
}